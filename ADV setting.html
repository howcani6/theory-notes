<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Adversarial Bandits & EXP3 — Full Notes</title>

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['$$', '$$']]
      }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    :root{
      --primary:#1e3a8a; --secondary:#2563eb; --accent:#f59e0b;
      --bg:#f8fafc; --text:#1e293b; --card:#ffffff;
      --muted:#64748b; --border:#e2e8f0;
    }
    *{box-sizing:border-box}
    body{
      font-family: system-ui, -apple-system, "Segoe UI", Roboto, "Noto Sans KR", sans-serif;
      background:var(--bg); color:var(--text); line-height:1.7;
      margin:0; padding:22px;
    }
    .container{max-width:1080px; margin:0 auto;}
    header{
      text-align:center; padding:50px 20px;
      background:linear-gradient(135deg, var(--primary), var(--secondary));
      color:#fff; border-radius:20px; margin-bottom:30px;
    }
    header h1{margin:0; font-size:2.2rem;}
    header p{margin:10px 0 0; opacity:.9;}

    section{
      background:var(--card); padding:0; border-radius:15px;
      margin-bottom:20px; box-shadow:0 4px 12px rgba(0,0,0,.05);
      border:1px solid var(--border); overflow: hidden;
    }
    details { width: 100%; }
    summary {
      list-style: none; padding: 22px 28px; cursor: pointer;
      outline: none; transition: background 0.2s ease;
    }
    summary:hover { background: #f1f5f9; }
    summary::-webkit-details-marker { display: none; }

    h2{
      display: flex; justify-content: space-between; align-items: center;
      color:var(--primary); border-left:5px solid var(--secondary);
      padding-left:14px; margin:0; font-size:1.4rem;
    }
    h2::after { content: '->'; font-size: 1.1rem; transition: transform 0.3s ease; color: var(--muted); }
    details[open] h2::after { transform: rotate(90deg); }

    .content { padding: 0 30px 30px 30px; }
    h3{color:var(--secondary); margin:24px 0 10px; font-size:1.15rem; border-bottom:1px solid var(--border); padding-bottom:5px;}
    h4{margin:18px 0 8px; font-size:1rem; color:#0f172a; font-weight:700;}

    .quote{
      background:#eff6ff; border-left:4px solid var(--secondary);
      padding:12px 16px; border-radius:10px; margin:15px 0; font-style:italic;
    }
    .formula{
      background:#f1f5f9; padding:15px; border-radius:10px;
      margin:15px 0; overflow-x:auto; border:1px solid var(--border);
    }
    .interpretation {
      background: #f8fafc; padding: 15px; border-radius: 10px;
      border: 1px dashed var(--secondary); margin: 15px 0;
    }
    .interpretation b { color: var(--secondary); }

    .step-box { border-left: 3px solid var(--accent); background: #fffcf5; padding: 15px; margin: 10px 0; border-radius: 0 10px 10px 0; }
    .footer{ text-align:center; color:var(--muted); font-size:.9rem; margin:40px 0; }
  </style>
</head>
<body>

<div class="container">
  <header>
    <h1>Adversarial Bandits → EXP3</h1>
    <p>oblivious vs adaptive, regret 정의, full-information → bandits(IPW), EXP3 preliminaries</p>
  </header>

  <section>
    <details>
      <summary><h2>1) Adversarial setup</h2></summary>
      <div class="content">

        <h3>(1) “repeated game”</h3>
        <ul>
          <li>라운드 \(t=1,2,\dots,T\)가 있고 매 라운드</li>
          <li>
            1. 학습자(알고리즘)가 행동 \(A_t\in[N]\) 선택<br/>
            2. 적대자(adversary)가 그 라운드의 보상벡터(or 손실벡터)를 정함
          </li>
        </ul>

        <p>슬라이드 첫 줄에 “algorithm selects \(A_t\) then adversary selects \((x_{t1},\dots,x_{tN})\)”라고 적혀 있는데,</p>
        <ul>
          <li><b>이 순서가 적대자의 힘을 가장 크게 주는 설정</b>이야(= adaptive adversary 쪽에 가까움).</li>
          <li>다만 문헌에 따라 “동시에 결정”으로 두거나, 분석은 “oblivious”로 제한하기도 해(뒤 슬라이드에서 구분).</li>
        </ul>

        <h3>(2) “보상 생성 가정 없음”</h3>
        <ul>
          <li>확률분포, i.i.d., stationary 같은 가정 없음.</li>
          <li>\(x_t=(x_{t1},\dots,x_{tN})\in[0,1]^N\)이 각 \(t\)에서 <b>아무렇게나</b> 정해질 수 있음.</li>
        </ul>

        <h3>(3) \(P_t\in\mathcal P_{N-1}\) 의 의미</h3>
        <div class="formula">
          $$\mathcal P_{N-1}=\left\{
          P_t=(p_{t1},\dots,p_{tN}) : p_{ti}\ge 0,\ \sum_{i=1}^N p_{ti}=1
          \right\}$$
        </div>

        <ul>
          <li>알고리즘은 “이번 라운드에 각 팔을 몇 %로 뽑겠다”를 먼저 정하고(\(P_t\))</li>
          <li>실제 행동은 \(A_t\sim P_t\)로 샘플링됨.</li>
        </ul>

        <div class="interpretation">
          <b> 왜 굳이 분포를 고르냐?</b><br/>
          적대적 환경에서는 결정론적(항상 같은 선택 규칙)이 박살나기 때문에, <b>무작위화</b>가 전략의 일부가 되어야 함.
        </div>

        <h3>(4) “받는 보상”</h3>
        <p>선택된 팔 \(A_t\)의 보상만 관측:</p>
        <div class="formula">
          $$\text{reward}=x_{t,A_t}$$
        </div>
        <p>(단, 뒤의 “full-information setting” 슬라이드에서는 전부 관측하는 다른 세팅을 따로 설명함)</p>

        <h3>(5) policy \(\pi\) 정의</h3>
        <div class="formula">
          $$\pi:([N]\times[0,1])^{*}\to \mathcal P_{N-1}$$
        </div>
        <ul>
          <li>\(([N]\times[0,1])^{*}\) : “과거 기록(history)”들의 집합</li>
        </ul>
        <div class="formula">
          $$h_{t-1}=\big((A_1,\text{obs}_1),\dots,(A_{t-1},\text{obs}_{t-1})\big)$$
        </div>
        <ul>
          <li>\(\pi(h_{t-1})=P_t\) : 과거를 보고 다음 라운드 분포를 내놓는 함수.</li>
        </ul>

      </div>
    </details>
  </section>

  <section>
    <details>
      <summary><h2>2) Oblivious vs. adaptive adversary</h2></summary>
      <div class="content">

        <p>여기서부터 “적대자가 <b>언제 무엇을 보고</b> 정하냐”를 구분함.</p>

        <h3>(1) reward 대신 loss를 많이 씀</h3>
        <div class="formula">
          $$y_{ti}=1-x_{ti}$$
        </div>
        <ul>
          <li>보상 최대화 = 손실 최소화로 바꾼 것뿐.</li>
          <li>\([0,1]\) 범위 유지가 편하고, 분석이 “합 손실” 형태로 깔끔해져서 손실 표기를 자주 씀.</li>
        </ul>

        <h3>(2) Oblivious adversary (비적응형)</h3>
        <ul>
          <li>핵심: <b>학습자의 미래 행동에 의존하지 않음.</b></li>
          <li>두 종류:</li>
        </ul>
        <div class="step-box">
          <div><b>Deterministic oblivious:</b> 전체 손실 시퀀스 \((y_1,\dots,y_T)\)를 라운드 1 시작 전에 고정</div>
          <div><b>Randomized oblivious:</b> 손실 시퀀스에 대한 분포를 미리 고정해두고, 시작 전에 한 번 샘플링해서 고정</div>
        </div>
        <p>즉, “게임 시작 전에 시나리오가 이미 정해져 있음”에 가까워.</p>

        <h3>(3) Adaptive (non-oblivious) adversary (적응형)</h3>
        <ul>
          <li>손실 \(y_t\)가 학습자의 <b>과거 행동/관측</b>에 의존 가능.</li>
          <li>예시 “웹 레이아웃”: 네 추천/배치가 사용자 행동을 바꾸고, 그 변화가 다음 라운드 손실에 영향을 줌 → 상호작용적 환경.</li>
        </ul>

      </div>
    </details>
  </section>

  <section>
    <details>
      <summary><h2>3) Concrete examples: oblivious vs adaptive</h2></summary>
      <div class="content">

        <h3>(A) Oblivious 예시: cyclic losses (N=3)</h3>
        <p>라운드마다 손실벡터가 주기적으로 돌아:</p>
        <ul>
          <li>\(t\equiv1\ (\text{mod }3)\): \(y_t=(0,1,1)\)</li>
          <li>\(t\equiv2\ (\text{mod }3)\): \(y_t=(1,0,1)\)</li>
          <li>\(t\equiv0\ (\text{mod }3)\): \(y_t=(1,1,0)\)</li>
        </ul>
        <ul>
          <li>매 라운드마다 “그 라운드의 최적 팔”이 정해져 있고 3라운드마다 돌아온다.</li>
          <li>학습자가 뭘 했든 이 시퀀스는 변하지 않음 → oblivious.</li>
        </ul>

        <h3>(B) Adaptive 예시: follow-the-leader trap</h3>
        <p>정의:</p>
        <div class="formula">
          $$y_{t,i}=
          \begin{cases}
          0 & i=A_{t-1}\\
          1 & i\ne A_{t-1}
          \end{cases}
          \qquad (t\ge2)$$
        </div>
        <p>초기 \(y_{1i}=1\) (모든 팔 손실 1로 시작)</p>

        <p>뜻:</p>
        <ul>
          <li>“<b>직전에 네가 고른 팔만</b> 이번 라운드 손실 0(좋음), 나머지는 1(나쁨)”으로 적이 세팅.</li>
        </ul>

        <h4>직관</h4>
        <ul>
          <li>\(t=1\): 전부 손실 1이라 동률 → 임의로 하나 고름(예: 1번)</li>
          <li>\(t=2\): 1번만 손실 0이라 “리더”가 1번이 됨 → 또 1번 고르게 유도</li>
          <li>적이 “네 과거 선택에 맞춰” 손실을 만들기 때문에 특정 알고리즘을 계속 끌고갈 수 있음.</li>
        </ul>

      </div>
    </details>
  </section>

  <section>
    <details>
      <summary><h2>4) Applications of Oblivious Adversaries</h2></summary>
      <div class="content">

        <p>공통점: “데이터가 <b>미리 고정</b>되어 있고, 네가 뭘 해도 미래 데이터 자체는 안 바뀜”</p>
        <ol>
          <li><b>고정 캠페인 온라인 광고(offline-like)</b><br/>클릭확률이 캠페인 데이터로 이미 정해져 있고, 네 입찰이 그 확률을 바꾸지 않는다고 가정.</li>
          <li><b>과거 시장 리플레이로 포트폴리오 선택</b><br/>과거 주가 수익률 시퀀스를 그대로 재생. 네가 어떤 포트폴리오를 골라도 “그 과거 가격”이 바뀌진 않음.</li>
          <li><b>추천 시스템의 오프라인 평가</b><br/>미리 기록된 사용자 피드백 로그로 평가. 지금 추천이 실제 사용자 행동을 바꾸는 “온라인 상호작용”은 없음.</li>
        </ol>

      </div>
    </details>
  </section>

  <section>
    <details>
      <summary><h2>5) Applications of Adaptive Adversaries</h2></summary>
      <div class="content">

        <p>공통점: “네 행동이 상대(환경/사람)의 다음 행동을 바꾼다.”</p>
        <ol>
          <li><b>실시간 웹 개인화</b><br/>네가 뭘 보여줬는지에 따라 사용자가 학습/피로/선호 변화 → 다음 손실이 과거 행동에 의존.</li>
          <li><b>보안/침입탐지</b><br/>공격자가 방어정책을 관찰하고 우회전략을 바꿈 → 적이 학습자의 정책에 반응.</li>
          <li><b>온라인 경매/가격결정</b><br/>경쟁자들이 네 과거 가격/입찰을 보고 대응 → 보상/손실 동학이 action-dependent.</li>
        </ol>

      </div>
    </details>
  </section>

  <section>
    <details>
      <summary><h2>6) Regret definitions</h2></summary>
      <div class="content">

        <p>여기가 중요. “환경 \(y\)”를 고정했을 때의 regret와, 최악 경우를 분리함.</p>

        <h3>(1) 환경 \(y\)</h3>
        <p>보통 \(y\)는 전체 손실 행렬을 의미:</p>
        <div class="formula">
          $$y \in [0,1]^{T\times N},\quad
          y={y_{t,i}}_{t\le T, i\le N}$$
        </div>

        <h3>(2) 한 정책 \(\pi\)의 regret \(R_T(\pi,y)\)</h3>
        <div class="formula">
          $$R_T(\pi,y)=
          \mathbb E\Big[\sum_{t=1}^T y_{t,A_t}\Big]
          \;-\;
          \min_{j\in[N]}\sum_{t=1}^T y_{t,j}$$
        </div>

        <ul>
          <li>왼쪽: 네가 실제로 낸 <b>기대 누적 손실</b></li>
          <li>오른쪽: 사후적으로(best in hindsight) 봤을 때 “항상 한 팔 \(j\)”만 썼다면 얻는 최소 누적 손실</li>
        </ul>

        <div class="interpretation">
          <b>비교대상</b>은 “매 라운드 다른 팔 고르는 초능력”이 아니라<br/>
          <b>한 개의 고정 팔(best fixed arm)</b>임.
        </div>

        <h3>(3) worst-case regret</h3>
        <div class="formula">
          $$R_T^*(\pi)=\sup_{y\in[0,1]^{T\times N}} R_T(\pi,y)$$
        </div>

        <h3>(4) “Deterministic policies cannot attain sublinear”</h3>
        <ul>
          <li>결정론이면 적이 너의 선택을 예측하고 매번 반대로 손실을 만들어서 선형 regret를 강제 가능:</li>
        </ul>
        <div class="formula">
          $$R_T^*(\pi)=\Omega(T)$$
        </div>
        <ul>
          <li>그래서 적대적 밴딧에서 sublinear worst-case regret(\(o(T)\), 보통 \(O(\sqrt{T})\))를 얻으려면 <b>무작위화가 필수</b>.</li>
        </ul>

      </div>
    </details>
  </section>

  <section>
    <details>
      <summary><h2>7) Full-information setting (all losses observed)</h2></summary>
      <div class="content">

        <p>여긴 “밴딧”이 아니라 “전문가 조언(experts) / 온라인 학습”에 가까운 세팅.</p>

        <h3>라운드 \(t\)</h3>
        <ol>
          <li>학습자: 분포 \(P_t\) 고르고 \(A_t\sim P_t\)</li>
          <li>적: 손실벡터 \(y_t=(y_{t1},\dots,y_{tN})\) 선택</li>
          <li>학습자: <b>모든 팔의 손실 \(y_{t1},\dots,y_{tN}\)를 전부 관측</b></li>
        </ol>

        <h3>밴딧과 차이</h3>
        <ul>
          <li>밴딧: \(y_{t,A_t}\) 하나만 봄</li>
          <li>full-info: \(y_t\) 전체를 봄 → 훨씬 쉬움</li>
        </ul>

        <h3>regret 식(슬라이드)</h3>
        <div class="formula">
          $$R_T=
          \sum_{t=1}^T \mathbb E[y_{t,A_t}]
          \;-\;
          \min_{j}\sum_{t=1}^T y_{t,j}$$
        </div>

        <ul>
          <li>형태는 같지만, <b>정보량</b>이 달라서 가능한 알고리즘/분석이 달라짐</li>
          <li>full-info: Hedge/Multiplicative Weights로 \(O(\sqrt{T\log N})\)</li>
          <li>bandit: EXP3로 \(O(\sqrt{TN\log N})\) 같이 더 불리해짐(대략 \(N\)이 추가)</li>
        </ul>

      </div>
    </details>
  </section>

  <section>
    <details>
      <summary><h2>From full information → bandits (IPW trick)</h2></summary>
      <div class="content">

        <p>이제 진짜 중요한 전환점.</p>

        <h3>(1) bandit에서는 뭐가 문제냐?</h3>
        <ul>
          <li>full-info: 모든 \(y_{t,i}\) 다 봄</li>
          <li>bandit: 내가 고른 \(A_t\)의 \(y_{t,A_t}\)만 봄</li>
        </ul>

        <div class="quote">
          “안 고른 버튼은 점수가 어땠을까?” 를 모름.
        </div>

        <h3>(2) 해결 아이디어: IPW (Inverse Probability Weighting)</h3>
        <p>정의</p>
        <div class="formula">
          $$\tilde y_{t,i}=
          \frac{\mathbf 1(A_t=i)\,y_{t,i}}{P_{t,i}}$$
        </div>

        <p>말로 풀면</p>
        <ul>
          <li>\(i\)번 버튼을 눌렀으면 실제 손실 \(y_{t,i}\)를 눌렀을 확률로 나눠서 크게 부풀림</li>
          <li>\(i\)번 버튼을 안 눌렀으면 0으로 둠</li>
        </ul>

        <h3>(3) 왜 이렇게 이상하게 하냐?</h3>
        <p>핵심 성질 (중요!)</p>
        <div class="formula">
          $$\mathbb E_{t-1}[\tilde y_{t,i}]=y_{t,i}$$
        </div>
        <p> 기대값 기준으로는 “정답” (unbiased)</p>

        <h4>예시</h4>
        <ul>
          <li>버튼 \(i\)를 누를 확률 = 0.2</li>
          <li>실제 손실 = 1</li>
        </ul>
        <p>그럼:</p>
        <ul>
          <li>20% 확률로 \(1/0.2=5\)를 기록</li>
          <li>80% 확률로 0 기록</li>
        </ul>
        <p>평균:</p>
        <div class="formula">
          $$0.2\times 5=1$$
        </div>
        <p> 평균적으로는 진짜 손실과 같음 (unbiased)</p>

      </div>
    </details>
  </section>

  <section>
    <details>
      <summary><h2>8) EXP3: preliminaries</h2></summary>
      <div class="content">

        <p>이제 full-info 알고리즘을 bandit용으로 개조</p>

        <h3>(1) 관측값 정리</h3>
        <p>보상:</p>
        <div class="formula">
          $$\hat x_{t,i}=
          \frac{\mathbf 1(A_t=i)\,x_{t,i}}{P_{t,i}}$$
        </div>

        <p>손실:</p>
        <div class="formula">
          $$\tilde y_{t,i}=
          \frac{\mathbf 1(A_t=i)\,y_{t,i}}{P_{t,i}}$$
        </div>

        <p> 둘 다 IPW 추정량</p>

        <h3>(2) 평균은 맞는데… 문제는?</h3>
        <p>분산이 큼. </p>

        <p>슬라이드:</p>
        <div class="formula">
          $$\mathrm{Var}_{t-1}(\hat x_{t,i})
          =
          \frac{x_{t,i}^2(1-P_{t,i})}{P_{t,i}}$$
        </div>

        <div class="interpretation">
          <b>\(P_{t,i}\) 작으면 → 분산 폭발</b><br/>
          그래서 EXP3에서는 확률 하한을 두거나 exploration을 섞음.
        </div>

        <h3>(3) 최종 EXP3 구조</h3>
        <p>\(\tilde y_{t,i}\)로 가짜 full-info 손실 만들고</p>

        <p>누적:</p>
        <div class="formula">
          $$\tilde L_{t,i}=\sum_{s=1}^t \tilde y_{s,i}$$
        </div>

        <p>그걸로 다시</p>
        <div class="formula">
          $$P_{t,i}\propto \exp\!\left(-\eta \tilde L_{t-1,i}\right)$$
        </div>

        <p> “full-info 알고리즘 + 가짜 데이터”</p>

      </div>
    </details>
  </section>

  <div class="footer">
    &copy; 2026 Bandits Notes
  </div>

</div>

</body>
</html>
