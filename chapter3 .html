<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Learning Theory — Chapter 3 (PAC Learning & General Model)</title>

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['$$', '$$']]
      }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    :root{
      --primary:#2563eb; --secondary:#3b82f6; --accent:#f59e0b;
      --bg:#f8fafc; --text:#1e293b; --card:#ffffff;
      --muted:#64748b; --border:#e2e8f0;
    }
    *{box-sizing:border-box}
    body{
      font-family: system-ui, -apple-system, "Segoe UI", Roboto, "Noto Sans KR", sans-serif;
      background:var(--bg); color:var(--text); line-height:1.7;
      margin:0; padding:22px;
    }
    .container{max-width:1080px; margin:0 auto;}
    header{
      text-align:center; padding:56px 22px;
      background:linear-gradient(135deg,#1e3a8a,#2563eb);
      color:#fff; border-radius:20px; margin-bottom:34px;
    }
    header h1{margin:0; font-size:2.35rem;}
    header p{margin:10px 0 0; opacity:.92;}

    section{
      background:var(--card); padding:0; border-radius:20px;
      margin-bottom:26px; box-shadow:0 6px 16px rgba(2,8,23,.06);
      border:1px solid rgba(226,232,240,.6);
      overflow: hidden;
    }
    details { width: 100%; }
    summary {
      list-style: none;
      padding: 24px 30px;
      cursor: pointer;
      outline: none;
      transition: background 0.2s ease;
    }
    summary:hover { background: #f1f5f9; }
    summary::-webkit-details-marker { display: none; }

    h2{
      display: flex;
      justify-content: space-between;
      align-items: center;
      color:var(--primary);
      border-left:5px solid var(--primary);
      padding-left:14px; margin:0;
      font-size:1.55rem;
    }
    h2::after {
      content: '->';
      font-size: 1.1rem;
      transition: transform 0.3s ease;
      color: var(--muted);
      font-weight: bold;
    }
    details[open] h2::after { transform: rotate(90deg); }

    .content { padding: 0 36px 36px 36px; }
    h3{color:var(--secondary); margin:22px 0 10px; font-size:1.15rem;}
    h4{margin:16px 0 8px; font-size:1.02rem; color:#0f172a;}
    .quote{
      background:#eff6ff; border-left:4px solid var(--secondary);
      padding:14px 18px; border-radius:12px; margin:14px 0;
      font-style:italic;
    }
    .formula{
      background:#f1f5f9; padding:16px 18px; border-radius:12px;
      margin:16px 0; overflow-x:auto; border:1px solid var(--border);
    }
    .warn{
      background:#fff7ed; border:1px solid #fed7aa; color:#9a3412;
      padding:10px 12px; border-radius:12px; margin:14px 0;
      font-weight:600;
    }
    .muted{color:var(--muted);}
    ul{margin:10px 0 0 20px;}
    li{margin:6px 0;}
    table{
      width:100%; border-collapse:collapse; margin:16px 0;
      overflow:hidden; border-radius:14px;
    }
    th,td{padding:12px; border:1px solid var(--border); text-align:left;}
    th{background:#f8fafc; color:var(--primary);}
    .footer{
      text-align:center; color:var(--muted);
      font-size:.9rem; margin:30px 0 10px;
    }
  </style>
</head>
<body>
<div class="container">

  <header>
    <h1>3. PAC Learning & General Model</h1>
    <p>PAC 정의의 수학적 약속 -> Agnostic PAC -> 손실 함수를 통한 일반화</p>
  </header>

  <section>
    <details>
      <summary><h2>3.1 PAC 정의: 2장과의 연결고리</h2></summary>
      <div class="content">
        <p>2장은 “왜 일반화가 확률적으로만 보장되는지”를 설명했고, 3장의 PAC 정의는 그것을 수식으로 ‘약속’한 것이다.</p>

        <h3>1) 2장에서 증명한 핵심 문장</h3>
        <div class="formula">
          $$\Pr_{S\sim D^m}[L_{(D,f)}(h_S) > \varepsilon] \le |\mathcal{H}|e^{-\varepsilon m}$$
        </div>
        <p><b>말로 번역하면:</b> “표본을 무작위로 뽑았을 때, ERM이 현실에서 나쁜 가설을 고를 확률은 표본 수(m)가 늘어나면 지수적으로 작아진다.”</p>
        <ul>
          <li>“확률적으로” ✔</li>
          <li>“\(\varepsilon\)보다 나쁘다” ✔</li>
          <li>“표본 수에 따라 제어된다” ✔</li>
          <li>-> PAC의 모든 요소가 이미 등장했다.</li>
        </ul>

        <h3>2) PAC 정의(Definition 3.1)의 공식화</h3>
        <div class="formula">
          $$\Pr_{S\sim D^m}[L_{(D,f)}(h_S) \le \varepsilon] \ge 1 - \delta$$
        </div>
        <p>이건 아까의 부등식을 뒤집어 쓴 것뿐이다:</p>
        <div class="formula">
          $$\Pr[L_{(D,f)}(h_S) > \varepsilon] \le \delta \iff \Pr[L_{(D,f)}(h_S) \le \varepsilon] \ge 1 - \delta$$
        </div>
        <p class="quote">“실패 확률을 \(\delta\) 이하로 만들 수 있으면, 그걸 PAC 학습이라고 부르자.”</p>

        <h3>3) \(\epsilon, \delta\)의 의미 재정의</h3>
        <ul>
          <li><b>\(\epsilon\) (Accuracy parameter):</b> “현실 기준으로 이 정도 이상 틀리면 나쁘다” (bad hypothesis 기준).</li>
          <li><b>\(\delta\) (Confidence parameter):</b> “이 정도 확률은 운 나쁜 경우로 허용하자” (ERM 실패 확률 상한).</li>
        </ul>

        <h3>4) Realizability 가정이 필요한 이유</h3>
        <p>2장에서 realizability를 가정했기 때문에 가능했던 핵심 논리:</p>
        <div class="formula">
          $$L_S(h^*) = 0 \Rightarrow \min_{h \in \mathcal{H}} L_S(h) = 0 \Rightarrow L_S(h_S) = 0$$
        </div>
        <p class="quote">“ERM 실패는 오직 ‘일반화 실패’ 때문이지, 모델이 표현을 못 해서가 아니다.”</p>

        <h3>5) Sample complexity (\(m_{\mathcal{H}}(\epsilon, \delta)\))</h3>
        <div class="formula">
          $$m \ge \frac{\log(|\mathcal{H}|/\delta)}{\varepsilon}$$
        </div>
        <p>PAC 정의에서 말하는 \(m_{\mathcal{H}}(\epsilon, \delta)\)는 정확히 말해 <b>“이 부등식을 만족시키는 최소 표본 수”</b>를 함수로 이름 붙인 것이다.</p>
      </div>
    </details>
  </section>

  <section>
    <details>
      <summary><h2>3.2 A More General Learning Model</h2></summary>
      <div class="content">
        <p class="quote">“현실에 맞게 학습 모델을 일반화하자.”</p>
        <ul>
          <li>(1) 정답 함수가 \(\mathcal{H}\) 안에 꼭 존재한다는 가정을 빼고(agnostic)</li>
          <li>(2) 분류(0/1)만이 아니라 회귀/다중분류 등까지 포괄하도록 손실(loss) 기반으로 다시 쓰자.</li>
        </ul>

        <h3>3.2.1 Agnostic PAC: Realizability 제거</h3>
        
        <h4>(A) \(D\)의 의미 변화</h4>
        <p>데이터는 이제 \((x,y) \sim D\)로 입력과 라벨이 함께 랜덤하게 나온다.</p>
        <div class="formula">$$D \text{ is a distribution over } \mathcal{X} \times \mathcal{Y}$$</div>
        <ul>
          <li>주변분포 \(D_X\): \(x\)가 얼마나 자주 나오나</li>
          <li>조건부분포 \(D(y|x)\): 주어진 \(x\)에서 라벨이 어떻게 흔들리나 (노이즈 반영)</li>
        </ul>

        <h4>(B) 진짜 리스크(Risk) 정의 변화</h4>
        <div class="formula">
          $$L_D(h) \stackrel{\text{def}}{=} \Pr_{(x,y) \sim D}[h(x) \neq y]$$
        </div>
        <p>-> 이제는 정답함수 \(f\)가 아니라, 데이터가 실제로 찍혀 나오는 라벨 \(y\)를 기준으로 틀림을 잰다.</p>

        <h4>(C) Bayes Optimal Predictor</h4>
        <p>이론적으로 도달 가능한 최소 오차를 달성하는 최적 분류기:</p>
        <div class="formula">
          $$f_D(x) = \begin{cases} 1 & \text{if } \Pr[y=1|x] \ge 1/2 \\ 0 & \text{otherwise} \end{cases}$$
        </div>
        <p class="warn">agnostic에서는 아무리 똑똑해도 최소 가능한 오차(Bayes Risk)가 0이 아닐 수 있다.</p>

        <h3>3.2.2 손실 함수(Loss Function)로의 일반화</h3>
        <p>분류, 회귀 등을 하나로 묶기 위해 예제 하나를 \(z\), 데이터 공간을 \(Z = \mathcal{X} \times \mathcal{Y}\)라 하자.</p>
        <div class="formula">$$\ell : \mathcal{H} \times Z \to \mathbb{R}_+$$</div>
        
        <ul>
          <li><b>진짜 리스크(기대 손실):</b> \(L_D(h) = \mathbb{E}_{z \sim D}[\ell(h,z)]\)</li>
          <li><b>경험 리스크(샘플 평균 손실):</b> \(L_S(h) = \frac{1}{m}\sum_{i=1}^m \ell(h,z_i)\)</li>
        </ul>

        <h4>손실 함수의 예시</h4>
        <table>
          <tr><th>종류</th><th>수식</th></tr>
          <tr><td>0-1 Loss (분류)</td><td>\(\ell_{0-1}(h,(x,y)) = \begin{cases} 0 & h(x)=y \\ 1 & h(x) \neq y \end{cases}\)</td></tr>
          <tr><td>Square Loss (회귀)</td><td>\(\ell_{sq}(h,(x,y)) = (h(x)-y)^2\)</td></tr>
        </table>
      </div>
    </details>
  </section>

  <section>
    <details>
      <summary><h2>Agnostic PAC 정의 (Definition 3.3 & 3.4)</h2></summary>
      <div class="content">
        <p>현실에서는 절대 오차가 0이 될 수 없으므로 성공 기준이 바뀐다.</p>
        <div class="formula">
          $$\Pr_{S \sim D^m}[L_D(h) \le \min_{h' \in \mathcal{H}} L_D(h') + \epsilon] \ge 1 - \delta$$
        </div>
        <p class="quote">“\(\mathcal{H}\) 안에서 가능한 최선(best-in-class)보다 \(\epsilon\)만큼만 더 나쁘지 않으면 성공.”</p>
        
        <ul>
          <li><b>Proper Learning:</b> 알고리즘 출력이 반드시 \(\mathcal{H}\) 안에 있어야 함.</li>
          <li><b>Improper Learning:</b> 출력이 \(\mathcal{H}\) 밖이어도 되지만, 성능은 위 기준을 만족해야 함.</li>
        </ul>
      </div>
    </details>
  </section>

  <div class="footer">
    &copy; 2026 Learning Theory Notes — Chapter 3
  </div>

</div>
</body>
</html>
